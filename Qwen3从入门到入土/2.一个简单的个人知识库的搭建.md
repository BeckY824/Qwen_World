# 一个简单的个人知识库问答助手的搭建

### 一.核心功能

1. **PDF文件**作为用户上传文档，将文档**向量化**并创建**知识库**(包含对文档进行简单的切分);
2. 选择知识库，**检索**用户提问的知识片段;
3. 提供知识片段与提问，获取**大模型回答**.

### 二.技术架构和工具

> 1. 框架：LangChain(问答链)
> 2. Embedding 模型：Qwen3-Embedding-0.6B
> 3. 向量数据库：Chroma
> 4. 大语言模型：Qwen3-4B-Instruct-2507
> 5. 环境推荐：Google Colab(T4 or L4 GPU) + Google Drive

① 需要提前加载的库：

- `pip install -U langchain-community`
- `pip install pymupdf`
- `pip install chromadb`
- `pip install transformers`

② 需要提前下载到本地的模型：

- [Qwen3-4B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507) (可能需要魔法)
- [Qwen3-Embedding-0.6B](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B) (可能需要魔法)

③ 你还需要准备一个PDF文档作为你的外搭知识库

### 三.具体实现

#### 1.准备工作

```python
#1.由于我是从Google Drive中调用模型，如果是本地调用则忽略这一步
from google.colab import drive
drive.mount('/content/drive')

#2.Import要使用的库
from langchain.vectorstores import Chroma #向量数据库
from langchain.document_loaders import PyMuPDFLoader #处理调用的PDF文件
from langchain.text_splitter import RecursiveCharacterTextSplitter #对调用的PDF进行分词
from langchain.embeddings.huggingface import HuggingFaceEmbeddings #调用准备的Embedding模型与HF适配
from langchain.llms import HuggingFacePipeline #调用准备的大模型与HF适配
from langchain.prompts import PromptTemplate #构造Prompt
from transformers import AutoModelForCausalLM, AutoTokenizer #调用大模型
import torch
```

#### 2.处理PDF文件

```python
# 加载 PDF
loaders_chinese = [
    PyMuPDFLoader("../Files/langyangbang.pdf") # 琅琊榜
]
docs = []
for loader in loaders_chinese:
    docs.extend(loader.load())
# 切分文档
text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=150)
split_docs = text_splitter.split_documents(docs)
```

#### 3.构建向量数据库

```python
persist_directory = "../Database/vector_db/chroma" #设置你的数据库的位置

vectordb = Chroma.from_documents(#适配数据库
    documents = split_docs, 
    embedding=embed_model,
    persist_directory=persist_directory,
)
vectordb.persist()

print(f"向量库中存储的数量：{vectordb._collection.count()}")
```

> 向量库中存储的数量：19911

#### 4.搭建本地LLM + 构造检索式回答链

```python
llm_modelpath = "/content/drive/MyDrive/Qwen3-4B-Instruct-2507" #替换你大模型本地的位置即可
tokenizer = AutoTokenizer.from_pretrained(llm_modelpath)
model = AutoModelForCausalLM.from_pretrained(llm_modelpath)
model = model.eval()
from transformers import pipeline
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300,
    return_full_text=False
)
llm = HuggingFacePipeline(pipeline=pipe)
```

> Device set to use cuda:0 
>
> 代表正在调用GPU运行

```python
# Build prompt
template = """仅基于以下检索到的上下文回答最后的问题：
- 不得引用或粘贴原文句子，不要包含上下文片段或出处。
- 如果不知道，请回答“不知道”。
- 用最多三句话、尽量简洁地作答。
- 结尾必须包含“感谢您的提问！”。

上下文：
{context}
问题：{question}
答案："""
QA_CHAIN_PROMPT = PromptTemplate.from_template(template)
```

```python
# 导入检索式回答链
from langchain.chains import RetrievalQA
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    return_source_documents=False,
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
)
```

#### 5.运用知识库回答助手

```python
question_1 = "梅长苏是谁的谋士?"
result = qa_chain({"query": question_1})
print("大模型+知识库后回答 question_1 的结果：")
print(result["result"])
```

> 大模型+知识库后回答 question_1 的结果： 梅长苏是靖王的谋士。感谢您的提问！

### 四.项目回顾

这样一个简单的个人知识库就搭建好了。

其中涉及的**难点**，最难的还是**硬件部分**，你需要一个CPU(16G)+GPU(22G)左右。虽然4B的模型相较于上百上千亿的模型已经很小了，但需要的算力还是挺高（对于一个普通人来说）。我跑了几次，整体在CPU RAM需求 4.7～7G，GPU上需求20.9G。

在LLM涉及的知识点包括：

- **文件的处理**(简单)
- **向量数据库**的运用(简单)
- 运用embedding和搭建**本地LLM**(下载耗时)
- 将所有串起来的**LangChain**(简单)

其实整个项目算是比较好入手，但如果想要更加深入的去体会，以下几个切入点是好的方向：

1. 文件处理时的切分(了解文档内容的特殊性，该如何有针对性的切分？切分的量该如何拿捏？有什么其他的切分方法？)
2. 运用Chroma向量数据库，我们只知道把文档存入了然后引用，是否可以可视化增加可解释性？
3. CPU和GPU的负荷这么高，如果使用低精度例如8FP，会带来多少释放的内存？
4. 尝试了QA_Chain，其他还有什么链可以尝试，是否可以让模型根据内容进行推理，而不是单纯的回答问题？



代码源项目来自于：https://github.com/aleiyoo/QA_Project_Learning/tree/main?tab=readme-ov-file