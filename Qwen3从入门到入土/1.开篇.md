# **大模型Qwen3-4B，从入门到入土 (一)**

## **一、世界狂奔的这几年**

这几年各行各业就一个字：卷。无论是汽车行业，通信行业还是电商行业的朋友，每周都在加班加点的处理各种工作。而对于那些做大模型的公司而言的卷，则是模型“越卷越大”。从当年 GPT-3 横空出世，一下子把参数拉到 1750 亿开始，大家就开始疯狂比拼。后来有了 GPT-4，各种70B、175B 的巨无霸模型接连登场。背后其实就是疯狂堆 GPU。我的AI引路人沐神就用"大力出奇迹"来比喻这些操作，想想也合理。

- **2020 年**：GPT-3 出来，175B 参数，直接震惊世界。
- **2022 年**：ChatGPT 爆火，大模型第一次走进大众生活。
- **2023–2024 年**：开源社区不甘落后，LLaMA、Mistral、Qwen 这些名字陆续刷屏，参数大小从 1B 到 100B，全都在往上堆。

问题也随之暴露出来：

- 模型太大，推理一次就要花钱，不是普通人能随便玩的。
- 本地电脑根本跑不动，只能依赖云端服务。
- 开发者和研究者门槛很高，想上手得先准备一台“显卡怪兽”。

就在大家都以为“大才是王道”的时候，小模型开始慢慢冒头：**它们也许没那么强大，但却实用、便宜、接地气也是最合适普通人去尝试的道路。**

## **二、小模型存在的意义**

我们不妨问一句：既然大模型更强，为何还需要小模型？

1. **可落地**

比如 Qwen3-4B，4B 参数量在 MacBook Air M 系列上就能跑起来，不需要租昂贵的 GPU 集群。这意味着：

- 创业团队能做原型实验

- 开发者能跑 demo、做本地化测试

2. **灵活性**

小模型适合做 **专用型任务**。比如：

- 针对一个行业做微调（法律、医疗、金融）

- 驱动一些轻量应用（对话机器人、多智能体模拟器）

3. **隐私与本地化**

不是所有数据都能上传到云端。小模型可以在本地运行，保证数据安全，尤其适合医疗、政务、金融这些对隐私敏感的领域。换句话说，小模型不是“大模型的低配版”，而是另一条发展路线：**更便宜、更轻量、更可控**。

## **三、Qwen3-4B 本地部署：小教程**

下面给一个极简的上手流程，适合在 **MacBook Air M 芯片 + 16GB 内存**环境下尝试。

### **1. 安装依赖**

```
pip install torch transformers accelerate
```

### **2. 加载模型**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen2-4B-Instruct" # 推荐自行在huggingface上下载模型，本地运行会更方便
tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"   # 在Mac上会自动用到 MPS
)

device = torch.device("mps") # 强制使用mps,速度会比cpu快 
model = model.to(device)

# prepare the model input
prompt = "你会说中文吗？"
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)
model_inputs = tokenizer([text], return_tensors="pt").to(device)

# conduct text completion
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=16384
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

content = tokenizer.decode(output_ids, skip_special_tokens=True)

print("content:", content)
```

### **3. 模型回复**

```python
content: 当然会！😊  
我不仅会说中文，还能和你用中文聊天、解决问题、写故事、做数学题，甚至帮你写简历或推荐书单！有什么我可以帮你的吗？🌟
```

以上的模型回复时间通常在7～15秒左右。在一台16GB的macbook air上是完全足够运行。

## 四、后续更新计划

从理论和实践两方面让读者了解什么是大模型以及到底该如何合理运用它。