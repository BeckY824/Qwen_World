# 训练和架构

这次我们粗略过一下 **Qwen3** 的训练和架构。

## 预训练&后训练

在模型能真正“聊天”“写文章”“写代码”之前，它要先经历一个「打怪升级」的过程，这就是所谓的 **预训练**。你可以把它想象成：在它参加社会工作之前，先送它去读很多书、刷很多题，让它从小白逐渐变得博学多才。

Qwen3 的**预训练**过程分为三个阶段：

1. 第一阶段使用约 30 万亿标记构建**通用知识基础**；
2. 第二阶段在**知识密集型数据**上训练，以强化模型在 STEM 与代码等领域的推理能力；
3. 第三阶段采用**长上下文数据**，将最大上下文长度从 4096 扩展至 32768 tokens。

> 1. **打基础（30 万亿标记）**
>
>    就像先让学生读完一个超级大的图书馆，包括百科、对话、新闻、小说……目的不是学精，而是先**建立通用的知识和语言理解力**。
>
> 2. **补强重点（STEM + 代码）**
>
>    STEM 是科学（Science）、技术（Technology）、工程（Engineering）和数学（Mathematics）这四个单词的首字母缩写。
>
>    有了通识教育后，第二阶段就像是给学生补课：重点上数学、理工科和编程。这样训练后，模型在逻辑推理、算式运算和写代码上会更强。
>
> 3. **扩展记忆（长上下文）**
>
>    最后阶段是让学生练习“长时间记忆”。之前模型最多能“记住”4096 个字，现在扩展到 32768 个字。相当于从只能看一篇短文，升级到能消化一本小书。这样它就能在长对话、长文档分析中保持条理清晰，不容易丢失上下文。

为提高与**人类偏好和下游应用的对齐度**，Qwen3 还进行了多阶段的**后训练**：前两阶段侧重于基于**长链思维（CoT）冷启动微调**和数学、编程任务的强化学习推理能力；后两阶段则将带有与**不带思维路径的数据合并统一微调**，并通过通用领域强化学习提升模型的综合表现。

> **人类偏好和下游应用的对齐度**：预训练后的模型就像一个读书无数的学生，知识很全，但说话方式、答题风格可能不太合人类口味。**对齐**就是调教它，学会“说人话”，比如回答要有条理、不乱跑题、语气自然，还要符合人类在不同应用场景（比如聊天、写代码、解数学题）里的期待。
>
> **长链思维（CoT）冷启动微调**：CoT（Chain of Thought）= 思维链，让模型在回答问题时，先“写草稿”，把推理过程一步步展开。**冷启动微调**就是在一开始给模型打个“强心针”：拿一批带思维过程的示例，专门训练它“先想后答”。
>
> - 比如：问模型“如果每个苹果 3 块钱，我买 4 个要多少钱？”
>   - 普通模型可能直接冒出“12”。
>   - CoT 模型会学到：每个苹果 3 块 → 4 个 × 3 = 12 → 答案是 12。
> - 这样它后续在数学、逻辑推理上更稳。
>
> **不带思维路径的数据合并统一微调**：不是所有任务都需要几步进行推理，在后续阶段会把两类数据混合。不带思维路径的数据让模型学会在简单人物直接给答案。

## Dense & MoE 架构

光有知识不够，还得有个好「大脑结构」。在大模型世界里，这就是 **架构**。Qwen3 提供了两种「大脑形态」：Dense（全员上阵型）和 MoE（专家分工型）。

**Dense架构**："dense" 指的是所有参数都会参与计算的设计。Qwen3 的 Dense 家族包括：`Qwen3-4B`、`Qwen3-8B`、`Qwen3-14B` 和 `Qwen3-32B`。

> 它的核心特点是：**上一层的任何一个节点，都必须和下一层的每一个节点全部连接起来。**
>
> 我们要把一个复杂的消息（比如一张图片的信息）从一年级传递到六年级，最后让六年级学生判断图片里是猫还是狗。
>
> - **年级 = 层 (Layer)**
>   - 一年级是**输入层**（接收最原始的信息）。
>   - 二到五年级是**中间层/隐藏层**（负责分析和传递）。
>   - 六年级是**输出层**（给出最终答案）。
> - **学生 = 节点 (Neuron)**
>   - 每个年级里的每个学生就是一个“节点”。
> - **“Dense/全连接”的规则**
>   - 这个学校的传话规则很特别：一年级的**每一个学生**，都必须分别把自己的信息告诉二年级的**所有学生**。
>   - 同样，二年级的**每一个学生**，在听完了一年级所有人的话、自己消化理解后，也必须把自己的新想法告诉三年级的**所有学生**。
>   - 这个过程一直持续到六年级。

**MoE（混合专家）架构**包括：`Qwen3-30B-A3B` 和 `Qwen3-235B-A22B`。

> MoE架构走的是另一条路：
>
> - 在同一个大模型的底子上，Qwen3 设计了 **128 个专家小组**。
> - 但每次处理一个 token（比如一个词），不会把 128 个专家全喊出来，而是只激活其中 **8 个专家**来干活。
>
> 这样既节省算力，又让模型在不同任务上表现更专业。



**Next：**我们会介绍预训练数据的构建过程、预训练方法的具体细节，并解释在标准基准上对基础模型进行评估的实验结果。